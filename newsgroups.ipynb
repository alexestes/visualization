{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "import string\n",
    "punctuation = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## install spacy\n",
    "\n",
    "### bash\n",
    "\n",
    "`pip install spacy`\n",
    "\n",
    "`python -m spacy download en`\n",
    "\n",
    "`python -m spacy download en_core_web_lg`\n",
    "\n",
    "### python\n",
    "\n",
    "`import spacy`\n",
    "\n",
    "`nlp = spacy.load(\"en\")`\n",
    "\n",
    "`nlp = spacy.load(\"en_core_web_lg\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "data = fetch_20newsgroups(subset='train')\n",
    "\n",
    "# make dataframe\n",
    "df = pd.DataFrame(data.data)\n",
    "\n",
    "# add targets\n",
    "df['target'] = data.target\n",
    "\n",
    "# rename text column\n",
    "df.rename(columns={0:'text'}, inplace=True)\n",
    "\n",
    "# add label names\n",
    "target_names = data.target_names\n",
    "df['label'] = df['target'].apply(lambda x: target_names[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[(df['label'] == 'rec.autos') | (df['label'] == 'sci.med') | (df['label'] == 'rec.sport.baseball')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### at this stage the df has 'text', 'target', and 'label' columns. the rest should be able to be done to any df provided it has these columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing\n",
    "\n",
    "add to built in stopwords\n",
    "\n",
    "lemmatization would be very helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make spacy docs\n",
    "\n",
    "df['spacy'] = df['text'].apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# custom stop words based off of sklearn\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS.union({\"edu\",\n",
    "                                               \"ca\",\n",
    "                                               \"com\",\n",
    "                                               \"gov\",\n",
    "                                               \"university\",\n",
    "                                               \"posting\",\n",
    "                                               \"line\",\n",
    "                                               \"lines\",\n",
    "                                               \"host\",\n",
    "                                               \"nntp\",\n",
    "                                               \"write\",\n",
    "                                               \"subject\",\n",
    "                                               \"organization\",\n",
    "                                               \"article\",\n",
    "                                               \"like\",\n",
    "                                               \"think\",\n",
    "                                               \"know\",\n",
    "                                               \"do\",\n",
    "                                               \"just\",\n",
    "                                               \"use\",\n",
    "                                               \"say\",\n",
    "                                               \"from\"\n",
    "                                                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lemmatize and filter out stop words\n",
    "def lemmatize(spacy_doc):\n",
    "    \n",
    "    lemmata = []\n",
    "    \n",
    "    for tok in spacy_doc:\n",
    "        if not tok.is_punct and tok.text not in punctuation and \"\\n\" not in tok.text:\n",
    "            if tok.lemma_ not in my_stop_words:\n",
    "                lemmata.append(tok.lemma_)\n",
    "            \n",
    "    return \" \".join(lemmata)\n",
    "\n",
    "df['lemmatized'] = df['spacy'].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(stop_words=my_stop_words,\n",
    "                            ngram_range = (2,4)\n",
    "                            )\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "counts = count_vect.fit_transform(df['lemmatized'])\n",
    "tfidf = tfidf_transformer.fit_transform(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 3\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "km.fit(tfidf)\n",
    "\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cluster'] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in df['label'].unique():\n",
    "    print(label)\n",
    "    print(df[df['label'] == label].cluster.value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create X (features) and y (targets)\n",
    "\n",
    "X = df['lemmatized']\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bernoulli naive bayes classifier\n",
    "used for binary features, i.e., word's occurrence in a document, _not_ its count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create vectorizer with stop words, optional to add ngrams\n",
    "\n",
    "count_vect = CountVectorizer(stop_words=my_stop_words, #stop_words='english'\n",
    "                            #ngram_range = (1,1)\n",
    "                            )\n",
    "\n",
    "# no tfidf because bernoulli does binary, tfidf relies on counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create count vectors\n",
    "counts = count_vect.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(counts, y, test_size=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit model\n",
    "bern_clf = BernoulliNB().fit(X_train, y_train) # use binary occurrence for bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get score on test data\n",
    "\n",
    "bern_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get top 10 features for each class and their weights\n",
    "# weights are log probabilities, which are negative since log of everything in the interval (0,1) is negative\n",
    "\n",
    "def get_top_10_features(vectorizer, clf, class_labels, class_names):\n",
    "    \n",
    "    top10_features = {}\n",
    "    \n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        \n",
    "        name = class_names[i]\n",
    "        \n",
    "        top10_weights = sorted(clf.coef_[i])[-10:]\n",
    "        top10_indices = np.argsort(clf.coef_[i])[-10:]\n",
    "        top10_names = [feature_names[j] for j in top10_indices]\n",
    "        \n",
    "        top10_features[name] = {n:w for n,w in zip(top10_names, top10_weights)}\n",
    "        \n",
    "    return top10_features\n",
    "        \n",
    "top10 = get_top_10_features(count_vect, # vectorizer\n",
    "                            bern_clf, # classifier\n",
    "                            bern_clf.classes_, # classes (as ints)\n",
    "                            target_names) # labels (as strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print out features\n",
    "\n",
    "for k,v in top10.items():\n",
    "    print(k)\n",
    "    features = list(sorted(v.items(), key=operator.itemgetter(1)))[::-1]\n",
    "    for k,v in features:\n",
    "        print(k, v)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multinomial naive bayes classifier\n",
    "used for word counts in each document, not simply occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(stop_words=my_stop_words,\n",
    "                            #ngram_range = (1,2)\n",
    "                            )\n",
    "tfidf_transformer = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts = count_vect.fit_transform(X)\n",
    "tfidf = tfidf_transformer.fit_transform(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tfidf, y, test_size=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mn_clf = MultinomialNB().fit(X_train, y_train) # use word counts for multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mn_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_top10(count_vect, mn_clf, mn_clf.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## old print top 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top10(vectorizer, clf, class_labels):\n",
    "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
    "    \n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        print(data.target_names[i])\n",
    "        top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "        print(\"%s: %s\" % (class_label,\n",
    "              \", \".join(feature_names[j] for j in top10)))\n",
    "        print()\n",
    "\n",
    "print_top10(count_vect, bern_clf, bern_clf.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ngrams visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bb = df[df['label'] == \"rec.sport.baseball\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make spacy docs\n",
    "\n",
    "bb['spacy'] = bb['text'].apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocess/clean docs\n",
    "\n",
    "def preprocess(spacy_doc):\n",
    "    '''\n",
    "    This function takes a spacy doc and filters out tokens that are punctuation, determiners, pronouns, numbers, etc.\n",
    "    It filters out stop words and returns the lemma of each remaining word.\n",
    "    The lemmata are rejoined to form a string.\n",
    "    '''\n",
    "    \n",
    "    cleaned = []\n",
    "    \n",
    "    bad_pos = ['PUNCT',\n",
    "              'SYM',\n",
    "              'X',\n",
    "              'NIL',\n",
    "              'PRON',\n",
    "              'SPACE',\n",
    "              'DET',\n",
    "              'NUM',\n",
    "              'PROPN']\n",
    "    \n",
    "    for token in spacy_doc:\n",
    "        if token.text not in punctuation:\n",
    "            if token.pos_ not in bad_pos:\n",
    "                if token.text not in my_stop_words and token.lemma_ not in my_stop_words:\n",
    "                    cleaned.append(token.lemma_)\n",
    "                \n",
    "    return \" \".join(cleaned)\n",
    "\n",
    "bb['cleaned'] = bb['spacy'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get bigrams and trigrams\n",
    "\n",
    "def find_bigrams(s):\n",
    "    \n",
    "    words = s.split()\n",
    "    \n",
    "    return list(zip(words, words[1:]))\n",
    "\n",
    "\n",
    "def find_trigrams(s):\n",
    "    \n",
    "    words = s.split()\n",
    "    \n",
    "    return list(zip(words, words[1:], words[2:]))\n",
    "\n",
    "# clever way to do ngrams\n",
    "def find_ngrams(input_list, n):\n",
    "    return list(zip(*[input_list[i:] for i in range(n)]))\n",
    "\n",
    "bb['bigrams'] = bb['cleaned'].apply(find_bigrams)\n",
    "bb['trigrams'] = bb['cleaned'].apply(find_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create lists of all bigrams and trigrams in the collection\n",
    "\n",
    "bigrams = []\n",
    "for b in bb['bigrams']:\n",
    "    bigrams.extend(b)\n",
    "    \n",
    "trigrams = []\n",
    "for t in bb['trigrams']:\n",
    "    trigrams.extend(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get bigram and trigram counts\n",
    "\n",
    "bigram_counts = {}\n",
    "\n",
    "for b in bigrams:\n",
    "    if b not in bigram_counts:\n",
    "        bigram_counts[b] = 1\n",
    "    else:\n",
    "        bigram_counts[b] += 1\n",
    "        \n",
    "trigram_counts = {}\n",
    "\n",
    "for t in trigrams:\n",
    "    if t not in trigram_counts:\n",
    "        trigram_counts[t] = 1\n",
    "    else:\n",
    "        trigram_counts[t] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sort bigrams and trigrams to get most frequent\n",
    "\n",
    "sorted_bigrams = sorted(bigram_counts.items(), key=operator.itemgetter(1))[::-1]\n",
    "sorted_trigrams = sorted(trigram_counts.items(), key=operator.itemgetter(1))[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word cloud of most frequent bigrams\n",
    "\n",
    "bigrams_string = \"\"\n",
    "\n",
    "for b in sorted_bigrams[:200]:\n",
    "    bigrams_string += b[0][0] + \"_\" + b[0][1] +  \" \"\n",
    "    \n",
    "wordcloud = WordCloud(max_font_size=80).generate(bigrams_string)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word cloud of most frequent trigrams\n",
    "\n",
    "trigrams_string = \"\"\n",
    "\n",
    "for t in sorted_trigrams[:200]:\n",
    "    trigrams_string += t[0][0] + \"_\" + t[0][1] +  \"_\" + t[0][2] + \" \"\n",
    "    \n",
    "wordcloud = WordCloud(max_font_size=80).generate(trigrams_string)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
